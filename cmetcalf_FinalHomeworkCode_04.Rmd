---
title: "cmetcalf_OriginalHomeworkCode_04"
author: "Cat Metcalf"
date: "2023-10-23"
output: 
prettydoc::html_pretty:
  theme: cayman
  toc: TRUE
---

# What's Your Malfunction?

## Question 1

Write a simple R function z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines:

-Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().

-When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().

-The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.

-The function should contain a check for the rules of thumb we have talked about (n∗p>5 and n∗(1−p)>5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.

-The function should return a list containing the members Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.

**Our function is going to look something like this:**
z.prop.test <- prop.test(x=c(sum(p1), sum(p2)), n= c(length(n1), length(n2)), conf.level = 0.95, alternative = c("two.sided", "less", "greater"), correct = FALSE)



Where prop.test is indicating we are using the proportion test function.
x= c(sum(p1), sum(p2)) is taking a vector of the sum of each estimated proportion to be considered if it is a 2 sample test, for one sample you would not run a vector but just take the sum of p1.
n= c(length(n1), length(n2)) is taking a vector of the sample sizes for a two sample, where for one sample you would only take the length of n1.
conf.level= 0.95 shows we want the 95% confidence level.
alternative= c("two.sided", "less", "greater") is showing the options you can apply with this argument, so it would depend on whether you want a two-tailed test or one-tailed, so you would indicate one of these choices depending.
correct = FALSE means that it using a large sample formula for the confidence interval, something usually applied for the possibility that data is not a normal distribution.

We can expect that this function will return a test statistic (shown by the notation "x-squared"), a p-value associated with the test statistic, a confidence interval, and sample estimates.

```{r}
z.prop.test <- function (p1, n1, p2, n2) {
  prop.test(x=c(sum(p1), sum(p2)), n= c(length(n1), length(n2)), conf.level = 0.95, alternative = c("two.sided", "less", "greater"), correct = FALSE)
}
```

```{r}
z.prop.test <- function(p1, n1, p2 = NULL, n2 = NULL, p0, p = NULL, conf.level=0.95, alternative= c("two.sided", "less", "greater")) {
   if(((n1*p1)>5)|| (n1*(1-p1) > 5)) {
     print("non-normal distribution in one sample setting")
   }
  if((n2*p2)> 5 || n2*(1-p2) > 5){
    print("non-normal distribution in two sample setting")
  }
  if(is.null(p2) == TRUE || is.null(n2) == TRUE){
    z <-(p1-p0)/sqrt(p0 *(1-p0)/n1) #one sample z stat
    lower <- p1 - qnorm(0.95) * sqrt(p1 * (1 - p1)/n1)
    upper <- p1 + qnorm(0.95) * sqrt(p1 * (1 - p1)/n1)
    ci <- c(lower, upper) #confidence interval
    ci
  } #this above section came from Reese in my peer commentor group, I liked her section of code here a lot better than what i was doing because it incomporated one of the stated arguments and made more sense to the function!
  if(!is.null(p2) & !is.null(n2)){ #this argument is also from Reese and I realized it made more sense to add it here for my code to actually conduct a two-sample and one-sample test with one function
      x <- cbind(p1, p2 = NULL)
      n <- cbind(n1, n2= NULL)
      pstar <- (sum(p1) + sum(p2))/(length(p1) + length(p2)) #pooled proportion
      phat1 <- mean(p1) #proportion of successes in sample group p1
      phat2 <- mean(p2) #proportion of successes in sample group p2
      pi <- 0 #expected difference in proportions between sample groups (set to zero)
      z <- (phat2 - phat1)/sqrt((pstar * (1 - pstar)) * (1/length(n1) + 1/length(n2))) #formula for two sample z-test
    } #conducting confidence interval tailed tests dependent on whether it is two tailed or one tailed
         if(alternative=="two.sided") {
            p <- 1 - pnorm(z, lower.tail = TRUE) + pnorm(z, lower.tail = FALSE)
         }
         if(alternative=="greater") {
            p <- 1 - pnorm(z, lower.tail = FALSE)
         }
         if(alternative=="less") {
           p <- pnorm(z, lower.tail = TRUE)
         }
  {
     crit <- qnorm(1 - alpha/2)  # identify critical values
     test <- p < -crit || p > crit  # boolean test
      CI <- phat + c( 
         -1*((qnorm(((1 - conf.level)/2) + conf.level))*SE.phat),
         ((qnorm(((1 - conf.level)/2) + conf.level))*SE.phat) )
    }
   return(list(estimate=z,p=p,CI=ci))
}
```
I used a one sample z.test created by someone as a template to work with forming my own function for this assignment since I struggled a lot with finding the correct syntax to use that was also easy to understand. It is found here: https://qualityandinnovation.com/2015/03/16/one-proportion-z-test-in-r/ .

Not entirely sure if this function is completely correct, but I gave it my best shot after working through it a few times and comparing with peers!

# Question 2

The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

-Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

-Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.

-Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.

-Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?

-Looking at your two models, which do you think is better? Why?

First we have to load in the dataset:
```{r}
library(curl)
```

```{r}
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN588_Fall23/KamilarAndCooperData.csv")
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
head(d) #the dataframe we are using
```
Next load in ggplot and manipulate:
```{r}
library(ggplot2)
library(ggpubr)
```

Using an example from Module 12 we can find a line of best fit for a regression model:

```{r}
model <- lm(MaxLongevity_m ~ Brain_Size_Species_Mean, data=d)
summary(model) #finds linear regression line of best fit; added the use of the summary() function to receive more information from the equation, referenced from Reese and the modules
#below code is references from Reese, with some changes to adhere more to my code, but what she did I had attempted to do previously but was unsuccessful.

beta1 <- cor(d$MaxLongevity_m, d$Brain_Size_Species_Mean, use = "complete.obs") * sd(d$MaxLongevity_m, na.rm = TRUE)/sd(d$Brain_Size_Species_Mean, na.rm = TRUE) #returns slope of linear regression
beta1

beta0 <- mean(d$MaxLongevity_m, na.rm = TRUE) - beta1 * mean(d$Brain_Size_Species_Mean, na.rm = TRUE)
beta0 #gives intercept of linear regression
#notice how beta1 and beta0 are really close to the estimates for intercept and Brain_Size_Species_Mean?
ci <- confint(model, level = 0.90)  # using the results of lm()
ci #gives 5% and 95%?

#back to Reese's code
pe <- beta1 * 800 + beta0
pe #point estimate

ci <- predict(nl, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "confidence",
    level = 0.9)  # confidence interval for a single value
ci

pi <- predict(nl, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction",
    level = 0.9)  # prediction interval for a single value
pi

#is there an easier way to plot these?

h_hat <- predict(model, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean))
df <- data.frame(cbind(d$Brain_Size_Species_Mean, d$MaxLongevity_m, h_hat))
names(df) <- c("x", "y", "yhat")
head(df) #had this section in my previous coding but was getting an error message that I fixed by seeing how Reese applied it!

#Using above technique we can see how we can create dataframes with our CI and PI values and then relabel them for plotting purposes, again referencing Reese's code since I liked how she applied this!
ci <- predict(nl, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "confidence",
    level = 0.95, )  # predicts CI for a predicted vector of values
head(ci)


df <- cbind(df, ci) #binds the list of CI values to dataframe
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr") #names the different columns containing CI values
head(df)


pi <- predict(nl, newdata = data.frame(Brain_Size_Species_Mean = d$Brain_Size_Species_Mean), interval = "prediction",
    level = 0.9)  # predicts PI values for the predicted vector of values
head(pi)

df <- cbind(df, pi)
names(df) <- c("x", "y", "yhat", "CIfit", "CIlwr", "CIupr", "PIfit", "PIlwr",
    "PIupr")
head(df)
```
```{r}
g <- ggplot(data = d, aes(x = MaxLongevity_m, y = Brain_Size_Species_Mean))
g <- g + geom_point()
g <- g + geom_smooth(method = "lm", formula = y ~ x)
g <- g + stat_regline_equation(label.x=30, label.y=800)
g <- g + stat_cor(aes(label=..rr.label..), label.x=30, label.y=750) #these last two lines came from my peer commentor and successfully added the equation to the plot, so thank you Erin! the first label places it on that section of the graph while the second half is what is giving the R^2 of the regression, with the label function actually placing it on the graph (This will be really helpful for my replication assignment!)
g <- g + geom_line(data = df, aes(x = x, y = PIlwr), colour = "red") + geom_line(data = df, aes(x = x, y = PIupr), colour = "red") + geom_point(alpha = 1/2) #adds PI upper and lower lines as well as CI upper, fit, and lower lines to plot of predicted data
#above addition is from Reese's code, where again we see how the previous work for assigning labels within the dataframes allows us to create lines on our plot, but I still wonder if there is an easier way to do this?
g <- g + geom_line(data = df, aes(x = x, y = CIlwr), colour = "blue") + geom_line(data = df, aes(x = x, y = CIupr), colour = "blue")
g #plots points with line of best fit
```
While I used Reese's code to replicate a way to calculate predicted values and confidence intervals, it did not apply correctly to my data as we can see by the plot. I am not sure if this is an error on my end with manipulating the code to my data or if this is showing the predicted values are completely off from the actual data. Also, Reese's section of plotting that applied lines for the confidence interval did not work in my code block, as it would not accept CIfit and CIlwr as objects even though I had assigned them within the dataframe.
(edit: to get them to work i had to create a new g <- g + line!)
Frankly, I have spent a lot of time on this section and tackling the second part/model overwhelmed me a lot, so for the sake of mental health I decided not to proceed further.

Challenges:
1. Writing the function to apply the arguments using the correct syntax was rather confusing, although my peer commentor's Erin and Reese helped a lot with this section and making it clearer (like if (argument credentials), etc.).
2. I am still confused if the function actually works with differentiating between a one-sample test and two-sample test, and it was hard to tell with other people's code because we used different variables for some things. That really showed how code can be difficult to understand in order to replicate (similar situation occurred with question 2)
3. Question 2 I felt more confident about but it was still rather difficult finding the right order to go in for applying predictions and confidence intervals. I did not want to just copy Reese's code (she did a really good job figuring this out) so I tried to work with it on my own end and find ways to apply her code with the work I already had, but I still ran into issues with replicating things. I also just felt confused with geom_text() since it did not seem to work for me and in fact my peer commentor Erin was able to give me a different solution that was easy to work with and learn how to place the equation on the plot!

This was a really difficult homework not in understanding the background of what was going on and how things should look, but applying it in the syntax of coding, so I really tried to give it my best and take on the challenge.